{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c8354a2b0b3912d89dec4b26bbae7be864a1ef4"
   },
   "source": [
    "# Intro\n",
    "In this notebook I am going to explore the data a little bit.\n",
    "The competition goal is to give probability estimation to if an image contains a cat or a dog.\n",
    "The content score is measured by by LogLoss, where\n",
    "* y=1 -> dog\n",
    "* y=0 -> cat\n",
    "\n",
    "We will try to minimize the score\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40714128a628b385e65eb9f593ce6d8b289cdcd9"
   },
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "004394ca442bbbf522796312e0ed1e605745933d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import regularizers\n",
    "from tqdm import tqdm      # a nice pretty percentage bar for tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10df0e18d3d231f1101fe3fb8ce7fdf36940bed0"
   },
   "source": [
    " Creating lists of image file names for both cats and dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "3f15e6b9978315d7ed089330dc7af48eb62ec81e"
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../input/train/' \n",
    "TEST_DIR = '../input/test/'\n",
    "\n",
    "image_list = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] \n",
    "dog_image_list = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "cat_image_list = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "test_image_list = [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "fb90f94e03922a749fb4283b7e2140a46a4fb66a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:\t 25000 \n",
      "Dog Images:\t 12500 \n",
      "Cat Images:\t 12500 \n",
      "TestImages:\t 12500\n"
     ]
    }
   ],
   "source": [
    "print('Total images:\\t',len(image_list),\n",
    "      '\\nDog Images:\\t',len(dog_image_list),\n",
    "      '\\nCat Images:\\t',len(cat_image_list),\n",
    "      '\\nTestImages:\\t',len(test_image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41125353e491da10cd24c8862b194307311ed660"
   },
   "source": [
    "We can see that there is equal number of images to cats and dogs\n",
    "\n",
    "Now we are going to load the images and resize them to 64x64x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "044571d70e4f923c42fc618ec12ba552273c0b57"
   },
   "outputs": [],
   "source": [
    "ROWS = 256\n",
    "COLS = 256\n",
    "CHANNELS = 3\n",
    "\n",
    "def load_image(file_path, size=(ROWS,COLS)):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "    b,g,r = cv2.split(img)\n",
    "    rgb_img = cv2.merge([r,g,b])\n",
    "    return cv2.resize(rgb_img, (ROWS,COLS))\n",
    "    \n",
    "\n",
    "def load_image_list(file_list, ret_labels=True):\n",
    "    count = len(file_list)\n",
    "    data = np.ndarray((count, ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "    labels = []\n",
    "    \n",
    "    for i, image_name in tqdm(enumerate(file_list)):\n",
    "        data[i] = load_image(image_name)\n",
    "        if ret_labels:\n",
    "            if 'dog' in image_name:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "68f1db10b97b4eaee2b2d2e1c82142fcc7c5064c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dogs' Images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12500it [00:48, 258.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Cats' Images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12500it [00:46, 268.35it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-09ca753a5af5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Normalize the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dog_image_list = dog_image_list[:1000]\n",
    "# cat_image_list = cat_image_list[:1000]\n",
    "\n",
    "print(\"Loading Dogs' Images...\")\n",
    "dog_data_images, dog_data_labels = load_image_list(dog_image_list)\n",
    "dog_train_images, dog_val_images, dog_train_labels, dog_val_labels = train_test_split(dog_data_images, dog_data_labels,\n",
    "                                                    test_size=0.2, random_state=2)\n",
    "\n",
    "\n",
    "print(\"Loading Cats' Images...\")\n",
    "cat_data_images, cat_data_labels = load_image_list(cat_image_list)\n",
    "cat_train_images, cat_val_images, cat_train_labels, cat_val_labels = train_test_split(cat_data_images, cat_data_labels,\n",
    "                                                   test_size=0.2, random_state=2)\n",
    "\n",
    "train_data = np.concatenate((dog_train_images, cat_train_images), axis=0)\n",
    "train_labels = dog_train_labels + cat_train_labels\n",
    "# train_labels = to_categorical(train_labels, num_classes=2)\n",
    "\n",
    "val_data = np.concatenate((dog_val_images, cat_val_images), axis=0)\n",
    "val_labels = dog_val_labels + cat_val_labels\n",
    "# val_labels = to_categorical(val_labels, num_classes=2)\n",
    "\n",
    "del cat_train_images\n",
    "del cat_val_images\n",
    "del dog_train_images\n",
    "del dog_val_images\n",
    "\n",
    "# Normalize the data\n",
    "train_data = train_data / 255.0\n",
    "val_data = val_data /255.0\n",
    "\n",
    "print (\"Training data shape: {}\".format(train_data.shape))\n",
    "print (\"Labels length: {}\".format(len(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e208359076406ccdc54992d37b1070e2e0bb47a"
   },
   "source": [
    "## Lets show some images\n",
    "sum dogs images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "148cb0e5c7184d0b8675ff095eaf7b80e99d7116"
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2,2)\n",
    "axarr[0,0].imshow(train_data[0])\n",
    "axarr[0,1].imshow(train_data[1])\n",
    "axarr[1,0].imshow(train_data[2])\n",
    "axarr[1,1].imshow(train_data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7baa592046cc21a84c53f29ec57eccf8f522b180"
   },
   "source": [
    "Some cats Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5888fe034a6f0b494414db53b68b1864c5d3627"
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2,2)\n",
    "axarr[0,0].imshow(train_data[1000])\n",
    "axarr[0,1].imshow(train_data[1001])\n",
    "axarr[1,0].imshow(train_data[1002])\n",
    "axarr[1,1].imshow(train_data[1003])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3e1992923b0db9fc99da517da4e060d17d48b9f"
   },
   "source": [
    "We can see that some images are not face closeup of the animal, but a whole body shots.\n",
    "\n",
    "We also go some strange images like dog.10801 (Finger in a metal loop???)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f36e69402295e111f76a3b30222c19ca44a25cf"
   },
   "outputs": [],
   "source": [
    "# strange_dog = load_image(TRAIN_DIR+'dog.10801.jpg', size=(350,261))\n",
    "# plt.imshow(strange_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acc8e682441b70e425f2fbbd59f19ebbaa7d657e"
   },
   "source": [
    "# Building simple classifier\n",
    "\n",
    "We are going to build a deep network for this task using keras and tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b9846f00b5fe229a62fd341ec0281426cf3d9b6"
   },
   "outputs": [],
   "source": [
    "regl2 = regularizers.l2(0.01)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# BLOC 1\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4), padding='Same', activation='relu', input_shape=(ROWS,COLS,CHANNELS)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4), padding='Same', activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4), padding='Same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "# BLOC 2\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), padding='Same', activation='relu'))\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), padding='Same', activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), padding='Same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "# BLOC 3\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4), padding='Same', activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4), padding='Same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "# BLOC 4\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=regl2))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=regl2))\n",
    "# model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(1, activation='sigmoid')) # check if we could use softmax function instead\n",
    "\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "076108ca9101a97ec985de1e1b8780c0faafc394"
   },
   "outputs": [],
   "source": [
    "# Save the checkpoint in the /output folder\n",
    "filepath = \"cat_dog_best_reg2_v2\"\n",
    "\n",
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                            monitor='val_loss',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='min')\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "history = model.fit(x=train_data, y=train_labels, batch_size=batch_size,\n",
    "                    epochs=epochs, verbose=2, validation_data=(val_data, val_labels),\n",
    "                   callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04e4a762c0cddf18c7e6b86a73530e78ab84d494"
   },
   "source": [
    "# Visualize the accuracy gain\n",
    "Let's plot the run history, and see if the model converge. In the first run we reached loss of 0.4122, we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4c64ddb69554fe02fd553bd8e4a28cfb29f7f2f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "# Loss Plot\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label='Validation Loss')\n",
    "\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "# Accuracy Plot\n",
    "ax[1].plot(history.history['acc'], color='b', label='Training Accuracy')\n",
    "ax[1].plot(history.history['val_acc'], color='r', label='Validation Accuracy')\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Test Images...\")\n",
    "test_data, _ = load_image_list(test_image_list, False)\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "print (\"Test data shape: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Test prediction shape: {}\".format(test_predictions.shape))\n",
    "with open('submission_file.csv','w') as f:\n",
    "    f.write('id,label\\n')\n",
    "\n",
    "print(\"Saving the prediction results...\")\n",
    "with open('submission_file.csv','a') as f:\n",
    "    for i, prediction in tqdm(enumerate(test_predictions)):\n",
    "        f.write('{},{}\\n'.format(i+1,prediction[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
